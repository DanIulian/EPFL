{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# Consult nltk to for tag description\n",
    "nltk.help.upenn_tagset('``')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('British', 'JJ'),\n",
       " ('Left', 'NNP'),\n",
       " ('Waffles', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('Falkland', 'NNP'),\n",
       " ('Islands', 'NNP')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = \"British Left Waffles on Falkland Islands\"\n",
    "\n",
    "text = headline.split()\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vocabulary size and variability reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus sample [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP')]\n",
      "There are 49815 unique words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "# Load corpus\n",
    "brown_tagged = brown.tagged_words(tagset=\"universal\")\n",
    "print(\"Corpus sample\", brown_tagged[0:10])\n",
    "\n",
    "# Find the number of unique words\n",
    "fdist_before = nltk.FreqDist(word.lower() for (word, tag) in brown_tagged)\n",
    "unique_num = len(fdist_before)\n",
    "print(\"There are {} unique words in the corpus.\".format(unique_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized corpus has size 46080.\n",
      "This leads to a percentage reduction of 7.497741644083107\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the corpus\n",
    "# converts a pos tag from the 'universal' format to wordnet format \n",
    "def get_wordnet_pos(universal_tag):\n",
    "    if universal_tag == 'VERB':\n",
    "        return wordnet.VERB\n",
    "    elif universal_tag == 'ADJ':\n",
    "        return wordnet.ADJ\n",
    "    elif universal_tag == 'ADV':\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    lemmas = {}\n",
    "    lemmas = set()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for (token, tag) in corpus:\n",
    "        # Convert univesal tag to wordnet tag\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(token, pos)\n",
    "        lemmas.add(lemma)\n",
    "    \n",
    "    return lemmas\n",
    "\n",
    "\n",
    "# Generate the lemmatized form of the corpus\n",
    "lemma_corp = lemmatize(brown_tagged)\n",
    "lemma_num = len(lemma_corp)\n",
    "print(\"The lemmatized corpus has size {}.\".format(lemma_num))\n",
    "print(\"This leads to a percentage reduction of {}.\".format((unique_num - lemma_num) / unique_num * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-gram tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Unigram tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus: 8872 sentences\n",
      "Size of training set: 7984 sentencces\n"
     ]
    }
   ],
   "source": [
    "#import the tagged and untagged sentences\n",
    "brown_tagged_sents = brown.tagged_sents(categories=['news', 'fiction'])\n",
    "print(\"Size of corpus: {0} sentences.\".format(len(brown_tagged_sents)))\n",
    "\n",
    "# split the sentences into training and test sets\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "print(\"Size of training set: {0} sentences.\".format(size))\n",
    "\n",
    "# Tagged test sentences (used as reference)\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "# Untagged test sentences\n",
    "raw_sents = brown.sents(categories=['news', 'fiction'])[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a unigram tagger\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tagging of the sentence:\n",
      "To Mark : `` Please give my regards to Myra '' .\n",
      "is\n",
      "[('To', 'TO'), ('Mark', 'NP'), (':', ':'), ('``', '``'), ('Please', 'VB'), ('give', 'VB'), ('my', 'PP$'), ('regards', None), ('to', 'TO'), ('Myra', None), (\"''\", \"''\"), ('.', '.')]\n",
      "\n",
      "The tagging of the sentence:\n",
      "She signed the letters quickly , stamped them , and placed them on the hall table for Raphael to mail in town .\n",
      "is\n",
      "[('She', 'PPS'), ('signed', 'VBD'), ('the', 'AT'), ('letters', 'NNS'), ('quickly', 'RB'), (',', ','), ('stamped', 'VBD'), ('them', 'PPO'), (',', ','), ('and', 'CC'), ('placed', 'VBN'), ('them', 'PPO'), ('on', 'IN'), ('the', 'AT'), ('hall', 'NN'), ('table', 'NN'), ('for', 'IN'), ('Raphael', None), ('to', 'TO'), ('mail', 'NN'), ('in', 'IN'), ('town', 'NN'), ('.', '.')]\n",
      "\n",
      "The tagging of the sentence:\n",
      "Then she went back to the wicker chair and resolutely adjusted her eyes to the glare on the water .\n",
      "is\n",
      "[('Then', 'RB'), ('she', 'PPS'), ('went', 'VBD'), ('back', 'RB'), ('to', 'TO'), ('the', 'AT'), ('wicker', None), ('chair', 'NN'), ('and', 'CC'), ('resolutely', None), ('adjusted', 'VBN'), ('her', 'PP$'), ('eyes', 'NNS'), ('to', 'TO'), ('the', 'AT'), ('glare', None), ('on', 'IN'), ('the', 'AT'), ('water', 'NN'), ('.', '.')]\n",
      "\n",
      "The performance of the tagger on the test set is 0.9336145169200588.\n",
      "The performance of the tagger on the test set is 0.8502823106037104.\n"
     ]
    }
   ],
   "source": [
    "# Inspect how the tagger behaves on some samples\n",
    "num_samples = 3\n",
    "for i in range(num_samples):\n",
    "    tags = unigram_tagger.tag(raw_sents[i])\n",
    "    print(\"The tagging of the sentence:\")\n",
    "    print(' '.join(raw_sents[i]))\n",
    "    print('is')\n",
    "    print(tags)\n",
    "    print()\n",
    "\n",
    "# Evaluate the tagger\n",
    "train_eval = unigram_tagger.evaluate(train_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(train_eval))\n",
    "test_eval = unigram_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words are left untagged because they were not encoutered in the training set, so the tagger has no idea about how it should tag them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Guesser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first test a default tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the tagger on the test set is 0.11230377861884966.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default tagger\n",
    "default_tagger = nltk.tag.DefaultTagger('NN')\n",
    "\n",
    "# Evaluate the default tagger\n",
    "test_eval = default_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the default tagger as a fallback for the unigram tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the tagger on the test set is 0.868834150276106.\n"
     ]
    }
   ],
   "source": [
    "# Create a unigram tagger with the default tagger as a fall-back\n",
    "unigram_default_tagger = nltk.UnigramTagger(train_sents, backoff=default_tagger)\n",
    "\n",
    "# Evaluate the baseline tagger\n",
    "test_eval = unigram_default_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the tagger on the test set is 0.2701495315505367.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the regular expression tagger\n",
    "regexp_tagger = nltk.tag.RegexpTagger(\n",
    "[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "(r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "(r'.*able$', 'JJ'),                # adjectives\n",
    "(r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "(r'.*ly$', 'RB'),                  # adverbs\n",
    "(r'.*s$', 'NNS'),                  # plural nouns\n",
    "(r'.*ing$', 'VBG'),                # gerunds\n",
    "(r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "(r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "\n",
    "# Evaluate the regexp tagger\n",
    "test_eval = regexp_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the regexp tagger as a fallback for the unigram tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the tagger on the test set is 0.8877582676676801.\n"
     ]
    }
   ],
   "source": [
    "# Create a unigram tagger with the default tagger as a fall-back\n",
    "unigram_regex_tagger = nltk.UnigramTagger(train_sents, backoff=regexp_tagger)\n",
    "\n",
    "# Evaluate the baseline tagger\n",
    "test_eval = unigram_regex_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Bigrams and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a bigram tagger\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tagging for the sentece:\n",
      "['I', 'did', 'not', 'object', 'to', 'the', 'object']\n",
      "is\n",
      "[('I', 'PPSS'), ('did', 'DOD'), ('not', '*'), ('object', 'VB'), ('to', 'TO'), ('the', None), ('object', None)]\n",
      "\n",
      "The performance of the tagger on the test set is 0.19631445058013278.\n"
     ]
    }
   ],
   "source": [
    "# Test the tagger on a sample sentence\n",
    "sentence = 'I did not object to the object'.split()\n",
    "print('The tagging for the sentece:')\n",
    "print(sentence)\n",
    "print('is')\n",
    "print(bigram_tagger.tag(sentence))\n",
    "print()\n",
    "\n",
    "# Test the tagger on the test set\n",
    "test_eval = bigram_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of the tagger with default backoff on the test set is 0.8848420922007818.\n",
      "The performance of the tagger with regex backoff on the test set is 0.9036421170192964.\n"
     ]
    }
   ],
   "source": [
    "# Test the bigram tagger with fallback on unigram taggers\n",
    "bigram_unigram_default_tagger = nltk.BigramTagger(train_sents, backoff=unigram_default_tagger)\n",
    "test_eval = bigram_unigram_default_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger with default backoff on the test set is {}.\".format(test_eval))\n",
    "\n",
    "bigram_unigram_regex_tagger = nltk.BigramTagger(train_sents, backoff=unigram_regex_tagger)\n",
    "test_eval = bigram_unigram_regex_tagger.evaluate(test_sents)\n",
    "print(\"The performance of the tagger with regex backoff on the test set is {}.\".format(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence, init_prob, trans_prob, emis_prob):\n",
    "    n = len(sentence)\n",
    "    nr_tags = len(init_prob)\n",
    "    rho = np.zeros((n, nr_tags))\n",
    "    prev_tag = np.zeros_like(rho)\n",
    "    res = []\n",
    "    \n",
    "    # Set the initial values\n",
    "    for i, ip in enumerate(init_prob):\n",
    "        rho[0, i] = ip * emis_prob[i, sentence[0]]\n",
    "    \n",
    "    # Compute the rest of the values, keeping track of the tags used\n",
    "    for i in range(1, n):\n",
    "        for t in range(nr_tags):\n",
    "            rho[i, t] = 0\n",
    "            for tprev in range(nr_tags):\n",
    "                new_p = rho[i - 1, tprev] * trans_prob[tprev, t]\n",
    "                if rho[i, t] < new_p:\n",
    "                    rho[i, t] = new_p\n",
    "                    prev_tag[i, t] = tprev\n",
    "            rho[i, t] *= emis_prob[t, sentence[i]]\n",
    "    \n",
    "    # Backtrack to form the answer\n",
    "    \n",
    "            \n",
    "    \n",
    "tagset = {'H': 0, 'T': 1}\n",
    "init_prob = [0.5, 0.5]\n",
    "trans_prob = [[0.4, 0.6], [0.9, 0.1]]\n",
    "emis_prob = [[0.49, 0.51], [0.85, 0.15]]\n",
    "sentence = list('HTTHTTHHTTHTTTHHTHHTTHTTTTHTHHTHTHHTTTH')\n",
    "sentence = list(map(lambda x: tagset[x]))\n",
    "vit = viterbi(sentence, init_prob, trans_prob, emis_prob)\n",
    "print('The most probable coin toss sequence that produced the string is',vit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
