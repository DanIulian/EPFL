{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    data = torch.empty(nb, 2).uniform_(-1, 1)\n",
    "    norms = data.norm(dim=1)\n",
    "    labels = (norms <= np.sqrt(2 / np.pi)).to(dtype=torch.long)\n",
    "    return data, labels\n",
    "\n",
    "def normalize(train, test):\n",
    "    mean, std = train.mean(0), train.std(0)\n",
    "    return (train - mean) / std, (test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train and test datasets\n",
    "NUM_SAMPLES = 1000\n",
    "train_data, train_labels = generate_disc_set(NUM_SAMPLES)\n",
    "test_data, test_labels = generate_disc_set(NUM_SAMPLES)\n",
    "\n",
    "# Normalize data\n",
    "train_data, test_data = normalize(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "        self.len = x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def train_model(model, train_input, train_target):\n",
    "    num_epochs = 250\n",
    "    lr = 0.1\n",
    "    batch_size = 100\n",
    "    # Build dataset and define data loader\n",
    "    dataset = Data(train_input, train_target)\n",
    "    dataloader = DataLoader(dataset, batch_size)\n",
    "    # Define loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define optimizer\n",
    "    optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in dataloader:\n",
    "            # Make predition\n",
    "            preds = model(x)\n",
    "            # Clear gradients\n",
    "            optim.zero_grad()\n",
    "            # Accumulate gradients\n",
    "            criterion(preds, y).backward()\n",
    "            # Optimize parameters\n",
    "            optim.step()\n",
    "        \n",
    "def compute_nb_errors(model, data_input, data_target):\n",
    "    batch_size = 100\n",
    "    # Build dataset and define data loader\n",
    "    dataset = Data(data_input, data_target)\n",
    "    dataloader = DataLoader(dataset, batch_size)\n",
    "    # Test model\n",
    "    nb_errors = 0\n",
    "    for x, y in dataloader:\n",
    "        _, preds = model(x).max(1)\n",
    "        nb_errors += (preds != y).sum()\n",
    "    return nb_errors.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2),\n",
    "        nn.Softmax()\n",
    "    )\n",
    "\n",
    "def create_deep_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(2, 4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 16),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(16, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2),\n",
    "        nn.Softmax()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.007\n"
     ]
    }
   ],
   "source": [
    "# Evaluate shallow model\n",
    "shallow_model = create_shallow_model()\n",
    "train_model(shallow_model, train_data, train_labels)\n",
    "nb_errors = compute_nb_errors(shallow_model, test_data, test_labels)\n",
    "print('Error rate:', nb_errors / test_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.063\n"
     ]
    }
   ],
   "source": [
    "# Evaluate deep model\n",
    "deep_model = create_deep_model()\n",
    "train_model(deep_model, train_data, train_labels)\n",
    "nb_errors = compute_nb_errors(deep_model, test_data, test_labels)\n",
    "print('Error rate:', nb_errors / test_data.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
